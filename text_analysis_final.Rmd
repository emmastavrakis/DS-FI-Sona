---
title: "Text analysis of SONA speeches"
date: "9/3/2018"
output: html_document
---

#1.) SETUP
```{r}
#Markdown setup
knitr::opts_chunk$set(echo=TRUE)

#Working directory
setwd("~/Documents/university_of_cape_town/data_science_for_industry")

#Load packages
library(tidytext)
library(tidyverse)
library(topicmodels)
library(wordcloud)

#Data import
txt_files <- list.files("~/Documents/university_of_cape_town/data_science_for_industry/sona-text-1994-2018")
```

#2.) PRE-PROCESSING DATA

```{r}
#Create the sona dataframe from the text files
sona <- data.frame(FILE=as.character(), SPEECH=as.character())  #Initialize matrix
for(i in txt_files){
  FILE <- paste0("~/Documents/university_of_cape_town/data_science_for_industry/sona-text-1994-2018/", i)
  #Import text as single character string (can also read.table but the "seperator" causes problems)
  this_speech <- readChar(FILE, nchars=file.info(FILE)$size)
  #Make data frame with metadata (filename contains year and pres) and speech
  this_sona <- data.frame(FILE=i, SPEECH=this_speech, stringsAsFactors=FALSE)
  #Make a single dataset
  sona <- rbind(sona, this_sona)
}

#Add some columns
sona$YEAR <- str_sub(sona$FILE, start=1, end=4)
sona$PRESIDENT <- sub('.*_', '', sona$FILE)
sona$PRESIDENT <- sub('*.txt', '', sona$PRESIDENT)

#Tokenize data (unnest_tokens)
sona_words <- sona %>% unnest_tokens(word, SPEECH, token="words", to_lower=TRUE) %>% filter(!word %in% stop_words$word)
sona_bigrams <- sona %>% unnest_tokens(BIGRAM, SPEECH, token="ngrams", n=2)
sona_trigrams <- sona %>% unnest_tokens(TRIGRAM, SPEECH, token="ngrams", n=3)
sona_sentences <- sona %>% unnest_tokens(SENTENCE, SPEECH, token="sentences")
```

#3.) TEXT MINING

```{r}
#Number of words in each speech
sona_words %>% group_by(FILE) %>%
               count() %>% 
               arrange(desc(n)) %>%
               head(5)
```
From the output it is clear that President Mbeki had the longest speeches with the most words in 5 of his speeches.


```{r}
#Plot the most commonly used words in a wordcloud
sona_words %>% count(word) %>%
               with(wordcloud(word, n, max.words=100))
```

The output above is a wordcloud plotted with the most common words appearing in the corpus of speeches. A lot of these words are not surprising and one would expect to see things like government, national, country etc.

A quick look into bigrams is now done.

```{r}
#Separate the bigrams 
bigrams_separated <- sona_bigrams %>%
                     separate(BIGRAM, c("word1", "word2"), sep = " ")

#Remove stop words
bigrams_filtered <- bigrams_separated %>%
                    filter(!word1 %in% stop_words$word) %>%
                    filter(!word2 %in% stop_words$word)

#Join up the bigrams again
bigrams_united <- bigrams_filtered %>%
                  unite(BIGRAM, word1, word2, sep = " ")

#Return most common bigrams
bigrams_filtered %>% count(word1, word2, sort=TRUE) %>% 
                     filter(rank(desc(n)) <= 10)


#Most common bigrams by president??
```

The output above shows the most common bigrams appearing in the corpus of speeches. The bigram South Africa appeared the most with 318 times in the corpus, once again this is what we would expect to see.



#4.) SENTIMENT ANALYSIS

Sentiment analysis is a technique widely used in the field of text mining/analysis. This technique involves linking each word in a piece of text with either a score or emotion that has previously been assigned to that word in a lexicon. There are two main lexicons used in this analysis namely the bing dictionary that links each word with a positive or negative indicator. The other lexicon used is that of the nrc dictionary which links each word with a emotion like anger or happiness. 

The emotions or positivity/negativity can then be aggregated up in order to poroduce a net sentiment of a complete document. The bing dictionary is implemented first.

```{r}
#Getting bing sentiments
word_sentiments <- sona_words %>% 
                   left_join(get_sentiments("bing")) %>%
                   select(word, sentiment, everything()) %>% 
                   mutate(sentiment=ifelse(is.na(sentiment), "neutral", sentiment)) %>%
                   rename(WORD=word, SENTIMENT=sentiment)

#Calculate net sentiments over entire speeches
speech_sentiments <- word_sentiments %>%
                     group_by(YEAR) %>%
                     summarize(NET_SENTIMENT=(sum(SENTIMENT=="positive") - sum(SENTIMENT=="negative")))
```

Below we return the most negative speeches from the corpus.
```{r}
#Return the most negative speeches and net sentiment score
sona_words %>% left_join(speech_sentiments) %>% 
               select(PRESIDENT, FILE, NET_SENTIMENT) %>%
               distinct(FILE, NET_SENTIMENT) %>%
               arrange(NET_SENTIMENT) %>%
               head(5)
```
From the ouput above it is clear that the most negative speech and president was that of Mandela in 1998 with a net sentiment score of 1.

```{r}
#Return most positive speeches and net sentiment score
sona_words %>% left_join(speech_sentiments) %>%
               select(PRESIDENT, FILE, NET_SENTIMENT) %>%
               distinct(FILE, NET_SENTIMENT) %>%
               arrange(desc(NET_SENTIMENT)) %>%
               head(5)
```

From the ouput above it is clear that Mbeki was the most postive with a net sentiment score of 204 in 2003.

We now plot the net sentiment score over time

```{r}
#Plot net sentiment over time
plot(x=speech_sentiments$YEAR, y=speech_sentiments$NET_SENTIMENT, type="l", ylab="SENTIMENT", xlab="YEAR", main="AVERAGE SPEECH SENTIMENT OVER TIME")
```

From the plot above it is clear that the sentiments over time fluctuated. The highest peaks can be found during the time periods when Mbeki was president this validates our earlier results. It once again clear that he was the most consistent president in his positve sentiments. 

There also seems to be a positive spike leading up to the FIFA 2010 world cup.

#Using nrc lexicon

The nrc dictionary is now used to identify which kind of emotions are most prevalent in the corpus of speeches.

```{r}
#Getting nrc sentiments
nrc_word_sentiments <- sona_words %>% left_join(get_sentiments("nrc")) %>%
                       select(word, sentiment, everything()) %>% 
                       rename(WORD=word, SENTIMENT=sentiment)

#Summarize the nrc sentiments
nrc_word_sentiments %>% select(SENTIMENT, YEAR, PRESIDENT) %>%
                        group_by(SENTIMENT) %>%
                        count() %>%
                        arrange(desc(n))
```

Interesting that positiveness is the most popular feelings in the sona speeches. The top two feelings coming through positive and trust. This could indicate a sense of sugar coating in Sona speeches, when the economy was performing poorly positve indicators are still coming through.


#5.) TOPIC MODELLING

In this section the technique of topic modelling is implemented. Topic modelling is build on a bag of words structure and attempts to identify I specified amount of topics in a corpus and then returns the most popular words found in those topics. 

First we start by implementing Latent Dirichlet Allocation at a word-topic probability level. This means that the topics are constructed at a word level. The corpus is tokenizeds to words and from there Latent Dirichlet Allocation is implemented.

```{r}
#Create document term matrix
sona_tdf <- sona_words %>%
            group_by(FILE, word) %>%
            count() %>%
            rename(WORD=word) %>%
            ungroup()

#Create the object
dtm_sona <- sona_tdf %>% cast_dtm(FILE, WORD, n)

#Estimate parameters of the topic model using LDA
set.seed(1)
sona_lda <- LDA(dtm_sona, k=2)

#Word topic probabilities
term <- as.character(sona_lda@terms)
topic1 <- sona_lda@beta[1,]
topic2 <- sona_lda@beta[2,]
sona_topics <- tibble(term=term, topic1=topic1, topic2=topic2)

#Getting data into tidy format, transform parameters into probabilities
sona_topics <- tidy(sona_lda, matrix="beta")

#Top 20 terms in each topic
top_terms <- sona_topics %>%
             group_by(topic) %>%
             top_n(15, beta) %>%
             ungroup() %>%
             arrange(topic, -beta)

#Plotting these terms
top_terms %>% mutate(term=reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill=factor(topic))) + geom_col(show.legend=FALSE) + facet_wrap(~ topic, scales="free") + coord_flip()
```

The plot above shows the most frequent words appearing in each topic. We can see that the words within the topics above are fairly similar.

Topic modelling has been implemented by specifying two topics. It has been decided that more than two topics is not adding any new information as the topics contain similar words even when expanding to more than two topics.

One of the benefits of Latent Dirichlet Allocation is that words can be identified with more than one topic. This is important to the problem at hand, as all the documents contains the same theme, one would expect there to be similar words throughout different topics.

We can now plot scores that show which words had the greatest differnence in beta values, in order to determine the extent to which a word belonged to a certain topic rather than a other topic.

```{r}
#Calculate log ratios (Filter out most common words)
beta_spread <- sona_topics %>%
               mutate(topic = paste0("topic", topic)) %>%
               spread(topic, beta) %>%
               filter(topic1 > 0.001 | topic2 > 0.001) %>%
               mutate(log_ratio = log2(topic2 / topic1))

#Plot log ratios
beta_spread %>% group_by(direction = log_ratio > 0) %>%
                top_n(15, abs(log_ratio)) %>%
                ungroup() %>%
                mutate(term = reorder(term, log_ratio)) %>%
                ggplot(aes(term, log_ratio)) + geom_col() + labs(y="Log2 ratio of beta in topic 2/topic 1") + coord_flip()
```

The plot above visualizes the words that have the greatest beta probability differences between topic 1 and topic 2. Ot seems that in topic 1 there seems to be social or human focus with words like human, peace, strengthening, reality, criminal. Topic 2 seems to contain more of a radical economic transformation agenda containing words like build, integrated, empowerment, include, construction.

This plot has been more valuable in trying to pick different topics from the corpus of speeches.

We now implement a LDA solution that looks at document topic probabilities, the output above shows the different speeches combined with a 0 indicating the speech/document is mostly relating to topic 1 or a zero meaning the speech is mostly relating with topic 2.  

```{r}
#Pre-procesing
sona <- as.tibble(sona)
sona$SPEECH <- as.character(sona$SPEECH)
sona$SPEECH_ID <- 1:nrow(sona) 
sona <- sona %>% select(FILE, SPEECH, SPEECH_ID)

#Remove stop words
tidy_sona <- sona %>% 
             unnest_tokens(word, SPEECH, token="words", to_lower=TRUE) %>%
             filter(!word %in% stop_words$word)

#Create document-term matrix
sona_tdf <- tidy_sona %>%
            group_by(SPEECH_ID, word) %>%
            count() %>%  
            ungroup() %>%
            rename(WORD=word)

#Initialize the object
dtm_sona <- sona_tdf %>% 
            cast_dtm(SPEECH_ID, WORD, n)

#Estimate parameters of the topic model using LDA
set.seed(1)
sona_lda <- LDA(dtm_sona, k=2)

#Produce gamma values for topics
sona_gamma <- sona %>% 
              left_join(tidy(sona_lda, matrix="gamma") %>% 
              mutate(SPEECH_ID=as.numeric(document)) %>%
              select(-document) %>%
              spread(key=topic, value=gamma, sep="_"))

#Summarize to see which topic each speech belongs to
sona_gamma %>% group_by(FILE) %>% 
               summarize(ntopic1 = sum(topic_1 > 0.5))
```

It is interesting to note that in 1994 there were two speeches one pre-election by de klerk which was assigned to topic 1, the topic involving more social aspects. The other speech post election by Mandela belonged to topic 2 which involved more economic aspects.
