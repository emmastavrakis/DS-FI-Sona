---
title: "Text analysis of SONA speeches"
author: "Jaco de Swardt"
date: "9/3/2018"
output: html_document
---

#1.) SETUP
```{r}
#Markdown setup
knitr::opts_chunk$set(echo=TRUE)

#Working directory
setwd("~/Documents/university_of_cape_town/data_science_for_industry")

#Load packages
library(tidytext)
library(tidyverse)
library(topicmodels)
library(wordcloud)

#Data import
txt_files <- list.files("~/Documents/university_of_cape_town/data_science_for_industry/sona-text-1994-2018")
```

#2.) PRE-PROCESSING DATA

```{r}
#Create the sona dataframe from the text files
sona <- data.frame(FILE=as.character(), SPEECH=as.character())  #Initialize matrix
for(i in txt_files){
  FILE <- paste0("~/Documents/university_of_cape_town/data_science_for_industry/sona-text-1994-2018/", i)
  #Import text as single character string (can also read.table but the "seperator" causes problems)
  this_speech <- readChar(FILE, nchars=file.info(FILE)$size)
  #Make data frame with metadata (filename contains year and pres) and speech
  this_sona <- data.frame(FILE=i, SPEECH=this_speech, stringsAsFactors=FALSE)
  #Make a single dataset
  sona <- rbind(sona, this_sona)
}

#Add some columns
sona$YEAR <- str_sub(sona$FILE, start=1, end=4)
sona$PRESIDENT <- sub('.*_', '', sona$FILE)
sona$PRESIDENT <- sub('*.txt', '', sona$PRESIDENT)

#Tokenize data (unnest_tokens)
sona_words <- sona %>% unnest_tokens(word, SPEECH, token="words", to_lower=TRUE) %>% filter(!word %in% stop_words$word)
sona_bigrams <- sona %>% unnest_tokens(BIGRAM, SPEECH, token="ngrams", n=2)
sona_trigrams <- sona %>% unnest_tokens(TRIGRAM, SPEECH, token="ngrams", n=3)
sona_sentences <- sona %>% unnest_tokens(SENTENCE, SPEECH, token="sentences")
```

#3.) TEXT MINING

```{r}
#Number of words in each speech
sona_words %>% group_by(FILE) %>%
               count() %>% 
               arrange(desc(n)) %>%
               head(5)
```
It is clear that Mandela had the longest speeches.


```{r}
#Plot the most commonly used words in a wordcloud
sona_words %>% count(word) %>%
               with(wordcloud(word, n, max.words=100))
```

```{r}
#Separate the bigrams 
bigrams_separated <- sona_bigrams %>%
                     separate(BIGRAM, c("word1", "word2"), sep = " ")

#Remove stop words
bigrams_filtered <- bigrams_separated %>%
                    filter(!word1 %in% stop_words$word) %>%
                    filter(!word2 %in% stop_words$word)

#Join up the bigrams again
bigrams_united <- bigrams_filtered %>%
                  unite(BIGRAM, word1, word2, sep = " ")

#Return most common bigrams
bigrams_filtered %>% count(word1, word2, sort=TRUE) %>% 
                     filter(rank(desc(n)) <= 10)


#Most common bigrams by president??
```

#4.) SENTIMENT ANALYSIS

```{r}
#Getting bing sentiments
word_sentiments <- sona_words %>% 
                   left_join(get_sentiments("bing")) %>%
                   select(word, sentiment, everything()) %>% 
                   mutate(sentiment=ifelse(is.na(sentiment), "neutral", sentiment)) %>%
                   rename(WORD=word, SENTIMENT=sentiment)

#Calculate net sentiments over entire speeches
speech_sentiments <- word_sentiments %>%
                     group_by(YEAR) %>%
                     summarize(NET_SENTIMENT=(sum(SENTIMENT=="positive") - sum(SENTIMENT=="negative")))
```

```{r}
#Return the most negative speeches and net sentiment score
sona_words %>% left_join(speech_sentiments) %>% 
               select(PRESIDENT, FILE, NET_SENTIMENT) %>%
               distinct(FILE, NET_SENTIMENT) %>%
               arrange(NET_SENTIMENT) %>%
               head(5)
```
From the ouput above it is clear that the most negative speech and president was that of FW de Klerk pre elections in 1994 with a score of -5.

```{r}
#Return most positive speeches and net sentiment score
sona_words %>% left_join(speech_sentiments) %>%
               select(PRESIDENT, FILE, NET_SENTIMENT) %>%
               distinct(FILE, NET_SENTIMENT) %>%
               arrange(desc(NET_SENTIMENT)) %>%
               head(5)
```

From the ouput above it is clear that Mbeki was the most postive with a net sentiment score of 204 in 2003.

```{r}
#Plot net sentiment over time
plot(x=speech_sentiments$YEAR, y=speech_sentiments$NET_SENTIMENT, type="l", ylab="SENTIMENT", xlab="YEAR", main="AVERAGE SPEECH SENTIMENT OVER TIME")
```


#Using nrc lexicon

```{r}
#Getting nrc sentiments
nrc_word_sentiments <- sona_words %>% left_join(get_sentiments("nrc")) %>%
                       select(word, sentiment, everything()) %>% 
                       rename(WORD=word, SENTIMENT=sentiment)

#Summarize the nrc sentiments
nrc_word_sentiments %>% select(SENTIMENT, YEAR, PRESIDENT) %>%
                        group_by(SENTIMENT) %>%
                        count() %>%
                        arrange(desc(n))
```

Interesting that positiveness seems to be the most popular in the sona speeches. The top two feelings coming through positive and trust. This could indicate a sense of sugar coating in Sona speeches.


#5.) TOPIC MODELLING

```{r}
#Create document term matrix
sona_tdf <- sona_words %>%
            group_by(FILE, word) %>%
            count() %>%
            rename(WORD=word) %>%
            ungroup()

#Create the object
dtm_sona <- sona_tdf %>% cast_dtm(FILE, WORD, n)

#Estimate parameters of the topic model using LDA
set.seed(1)
sona_lda <- LDA(dtm_sona, k=2)

#Word topic probabilities
term <- as.character(sona_lda@terms)
topic1 <- sona_lda@beta[1,]
topic2 <- sona_lda@beta[2,]
sona_topics <- tibble(term=term, topic1=topic1, topic2=topic2)

#Getting data into tidy format, transform parameters into probabilities
sona_topics <- tidy(sona_lda, matrix="beta")

#Top 20 terms in each topic
top_terms <- sona_topics %>%
             group_by(topic) %>%
             top_n(15, beta) %>%
             ungroup() %>%
             arrange(topic, -beta)

#Plotting these terms
top_terms %>% mutate(term=reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill=factor(topic))) + geom_col(show.legend=FALSE) + facet_wrap(~ topic, scales="free") + coord_flip()
```

The plot above shows the most frequent words used by each topic. We can see that the words within the topics above are fairly similar.

One of the benefits of Latent Dirichlet Allocation is that words can be identified with more than one topic. This is important to the problem at hand, as all the documents contains the same theme, one would expect there to be similar words throughout different topics.

```{r}
#Calculate log ratios (Filter out most common words)
beta_spread <- sona_topics %>%
               mutate(topic = paste0("topic", topic)) %>%
               spread(topic, beta) %>%
               filter(topic1 > 0.001 | topic2 > 0.001) %>%
               mutate(log_ratio = log2(topic2 / topic1))

#Plot log ratios
beta_spread %>% group_by(direction = log_ratio > 0) %>%
                top_n(15, abs(log_ratio)) %>%
                ungroup() %>%
                mutate(term = reorder(term, log_ratio)) %>%
                ggplot(aes(term, log_ratio)) + geom_col() + labs(y="Log2 ratio of beta in topic 2/topic 1") + coord_flip()
```

The plot above visualizes the words that have the greatest beta probability differences between topic 1 and topic 2. For example we can see 2010 at the bottom, it is clear that 2010 (Potentially refering to the 2010 world cup) was a frequent word in topic 2 exponentially more so than in topic 1. This could be a indication that the topics could be split in different time frames. Where in the one time frame 2010 was a hot topic and in the other not.

#social vs economic

```{r}
#Pre-procesing
sona <- as.tibble(sona)
sona$SPEECH <- as.character(sona$SPEECH)
sona$SPEECH_ID <- 1:nrow(sona) 
sona <- sona %>% select(FILE, SPEECH, SPEECH_ID)

#Remove stop words
tidy_sona <- sona %>% 
             unnest_tokens(word, SPEECH, token="words", to_lower=TRUE) %>%
             filter(!word %in% stop_words$word)

#Create document-term matrix
sona_tdf <- tidy_sona %>%
            group_by(SPEECH_ID, word) %>%
            count() %>%  
            ungroup() %>%
            rename(WORD=word)

#Initialize the object
dtm_sona <- sona_tdf %>% 
            cast_dtm(SPEECH_ID, WORD, n)

#Estimate parameters of the topic model using LDA
set.seed(1)
sona_lda <- LDA(dtm_sona, k=2)

#Produce gamma values for topics
sona_gamma <- sona %>% 
              left_join(tidy(sona_lda, matrix="gamma") %>% 
              mutate(SPEECH_ID=as.numeric(document)) %>%
              select(-document) %>%
              spread(key=topic, value=gamma, sep="_"))

#Summarize to see which topic each speech belongs to
sona_gamma %>% group_by(FILE) %>% 
               summarize(ntopic1 = sum(topic_1 > 0.5))
```


