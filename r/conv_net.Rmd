---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r}

rm(list=ls())
library(tidyverse)

# point to where the txt files are
txt_files <- list.files("~/dsfi/sona-text-1994-2018/")
```


```{r}
sona <- data.frame(filename = as.character(), speech = as.character())
for(i in txt_files){
  file_name <- paste0("~/dsfi/sona-text-1994-2018/", i)
  
  # import text as single character string (can also read.table but the "seperator" causes problems)
  this_speech <- readChar(file_name, 
                          nchars = file.info(file_name)$size)
  
  # make data frame with metadata (filename contains year and pres) and speech
  this_sona <- data.frame(filename = i, speech = this_speech, stringsAsFactors = FALSE)
  
  # make a single dataset
  sona <- rbind(sona, this_sona)
}


```

```{r}
# extract year, testing

str_extract(sona$filename, "[0-9]{4}")

sona$year<-str_sub(sona$filename, start=1,end=4)
# exercise: extract president name
gsub('(.*)_\\w+', '\\1', sona$filename)

sub('.*_', '.*', sona$filename)

sona$which_pres <- sub('.*_', '', sona$filename)
sona$which_pres <- sub('*.txt', '', sona$which_pres)

sona$which_pres <- gsub('\\s+', '', sona$which_pres)



```






```{r}
library(tidytext)

# word tokenization

tidy_sona <- sona %>% unnest_tokens(word, speech, token = "words", to_lower = T) %>% 
  filter(!word %in% stop_words$word) 


# we want to predict sentences, so we need to first split into sentences
# add an ID variable for sentences and tokenize each sentence by words
tidy_sentences <- sona %>% 
  unnest_tokens(sentence, speech, token = "sentences", to_lower = T) %>%
  rowid_to_column("ID")



```


```{r}
tidy_sentences<-tidy_sentences[,c(-2,-3)]
speech_bigrams <- tidy_sentences %>%
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2)




speech_bigrams %>%
  count(bigram, sort = TRUE)

```

As one might expect, a lot of the most common bigrams are pairs of common (uninteresting) words, such as of the and to be: what we call ???stop-words??? (see Chapter 1). This is a useful time to use tidyr???s separate(), which splits a column into multiple based on a delimiter. This lets us separate it into two columns, ???word1??? and ???word2???, at which point we can remove cases where either is a stop-word.
```{r}
library(tidyr)

bigrams_separated <- speech_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united[1:10,]
```

The statistic tf-idf is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites.

```{r}
bigram_tf_idf <- bigrams_united %>%
  count(which_pres, bigram) %>%
  bind_tf_idf(bigram, which_pres, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf

bigrams_united %>%
  count(which_pres, bigram) %>%
  bind_tf_idf(bigram, which_pres, n) %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
  group_by(which_pres) %>% 
  top_n(15) %>% 
  ungroup %>%
  ggplot(aes(bigram, tf_idf, fill = which_pres)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~which_pres, ncol = 2, scales = "free") +
  coord_flip()
```

which are consecutive sequences of 3 words. We can find this by setting n = 3:

```{r}
trigrams<-tidy_sentences %>%
  unnest_tokens(trigram, sentence, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
unite(trigram, word1, word2, word3, sep = " ")


trigrams[1:10,]
```



```{r}
trigrams %>%
  count(which_pres, trigram) %>%
  bind_tf_idf(trigram, which_pres, n) %>%
  arrange(desc(tf_idf)) %>%
  mutate(trigram = factor(trigram, levels = rev(unique(trigram)))) %>% 
  group_by(which_pres) %>% 
  top_n(10) %>% 
  ungroup %>%
  ggplot(aes(trigram, tf_idf, fill = which_pres)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~which_pres, ncol = 2, scales = "free") +
  coord_flip()
```

```{r}

```


```{r}
max_features <- 2000        # choose max_features most popular words
minlen <- 5                # exclude sentences shorter than this
maxlen <- 300             # longest tweet (for padding)
embedding_dims <- 10       # number of dimensions for word embedding
```

Here we use Keras to tokenize the sentences - this turns each tweet into a vector of integers, each integer representing a word. 

```{r}
tokenizer = text_tokenizer(num_words = max_features)
fit_text_tokenizer(tokenizer, tidy_sentences$sentence)
sequences = tokenizer$texts_to_sequences(tidy_sentences$sentence)
length(sequences)
```


```{r}


tidy_sentences<-as.tibble(tidy_sentences)
tidy_sentences$which_pres<-as.numeric(as.factor(tidy_sentences$which_pres))

train_y_mat<-to_categorical(tidy_sentences$which_pres)
train_y_mat<-train_y_mat[,-1]
nrow(train_y_mat)
test <- list()
train <- list()

train_id <- sample(1:length(sequences),
                size = 0.8*length(sequences), 
                replace=F)

nrow(train$y)
test$x <-  sequences[-train_id]
train$x <- sequences[train_id]

train$y <- train_y_mat[train_id,]
test$y <-  train_y_mat[-train_id,]

table(train$y)
```

Sequences are of different length. We "pad" the shorter sequences with zeros so that all padded sequences are the same length.

```{r}
x_train <- train$x %>% pad_sequences(maxlen = maxlen)
x_test <- test$x %>% pad_sequences(maxlen = maxlen)
```


We can now define the model

```{r}
model <- keras_model_sequential()
```

```{r}
model %>% 
  # embedding layer maps vocab indices into embedding_dims dimensions
  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
  # add some dropout
  layer_dropout(0.5) %>%
  # convolutional layer
  layer_conv_1d(
    filters = 100,
    kernel_size = 3, 
    padding = "valid",  # "valid" means no padding, as we did it already
    activation = "relu", 
    strides = 2 ) %>%
    layer_global_max_pooling_1d() %>%
  layer_dense(128) %>%
  layer_activation("relu") %>%
  
  layer_dense(128) %>%
  layer_dropout(0.5) %>%
  layer_activation("relu") %>%
  
  layer_dense(6) %>%   # single unit output layer
  layer_activation("softmax")
```


```{r}
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)
```

Train and evaluate the model:

```{r}
model %>%
  fit(
    x_train, train$y,
    batch_size = 32,
    epochs = 10,
    validation_data = list(x_test, test$y)
  )
```


#### Hyperparameter tuning
Tuning hyperparameters for deep neural network is difficult as it is slow to train a deep neural network and there are numerours parameters to configure. In this part, we briefly survey the hyperparameters for convnet.

##### Learning rate
Learning rate controls how much to update the weight in the optimization algorithm. We can use fixed learning rate, gradually decreasing learning rate, momentum based methods or adaptive learning rates, depending on our choice of optimizer such as SGD, Adam, Adagrad, AdaDelta or RMSProp.

##### Number of epochs
Number of epochs is the the number of times the entire training set pass through the neural network. We should increase the number of epochs until we see a small gap between the test error and the training error.

##### Batch size
Mini-batch is usually preferable in the learning process of convnet. A range of 16 to 128 is a good choice to test with. We should note that convnet is sensitive to batch size.

##### Activation function
Activation funtion introduces non-linearity to the model. Usually, rectifier works well with convnet. Other alternatives are sigmoid, tanh and other activation functions depening on the task.

##### Number of hidden layers and units
It is usually good to add more layers until the test error no longer improves. The trade off is that it is computationally expensive to train the network. Having a small amount of units may lead to underfitting while having more units are usually not harmful with appropriate regularization.


##### Dropout for regularization
Dropout is a preferable regularization technique to avoid overfitting in deep neural networks. The method simply drops out units in neural network according to the desired probability. A default value of 0.5 is a good choice to test with.

