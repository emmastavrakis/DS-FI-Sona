layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
# add some dropout
layer_dropout(0.5) %>%
# convolutional layer
layer_conv_1d(
filters = 100,
kernel_size = 3,
padding = "valid",  # "valid" means no padding, as we did it already
activation = "relu",
strides = 2 ) %>%
layer_global_max_pooling_1d() %>%
layer_dense(128) %>%
layer_activation("relu") %>%
layer_dense(128) %>%
layer_dropout(0.5) %>%
layer_activation("relu") %>%
layer_dense(6) %>%   # single unit output layer
layer_activation("softmax")
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
cnn_history <- cnn_model %>%
fit(
x_train, train$y,
batch_size = 400,
epochs = 10,
validation_data = list(x_test, test$y)
)
plot(history)
plot(cnn_history)
summary(cnn_model)
cnn_model %>% evaluate(x_test, test$y)
cnn_model %>%
# embedding layer maps vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
# add some dropout
layer_dropout(0.5) %>%
# convolutional layer
layer_conv_1d(
filters = 100,
kernel_size = 3,
padding = "valid",  # "valid" means no padding, as we did it already
activation = "relu",
strides = 2 ) %>%
layer_global_max_pooling_1d() %>%
layer_dense(128) %>%
layer_activation("relu") %>%
layer_dense(128) %>%
layer_dropout(0.5) %>%
layer_activation("relu") %>%
layer_dense(6) %>%   # single unit output layer
layer_activation("softmax")
summary(cnn_model)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "RMSProp",
metrics = "accuracy"
)
cnn_history <- cnn_model %>%
fit(
x_train, train$y,
batch_size = 400,
epochs = 10,
validation_data = list(x_test, test$y)
)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "RMSProp",
metrics = "accuracy"
)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "Adam",
metrics = "accuracy"
)
?compile
cnn_model <- keras_model_sequential()
cnn_model %>%
# embedding layer maps vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
# add some dropout
layer_dropout(0.5) %>%
# convolutional layer
layer_conv_1d(
filters = 100,
kernel_size = 3,
padding = "valid",  # "valid" means no padding, as we did it already
activation = "relu",
strides = 2 ) %>%
layer_global_max_pooling_1d() %>%
layer_dense(128) %>%
layer_activation("relu") %>%
layer_dense(128) %>%
layer_dropout(0.5) %>%
layer_activation("relu") %>%
layer_dense(6) %>%   # single unit output layer
layer_activation("softmax")
summary(cnn_model)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "Adam",
metrics = "accuracy"
)
?compile
cnn_history <- cnn_model %>%
fit(
x_train, train$y,
batch_size = 400,
epochs = 10,
validation_data = list(x_test, test$y)
)
cnn_model <- keras_model_sequential()
cnn_model %>%
# embedding layer maps vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
# add some dropout
layer_dropout(0.5) %>%
# convolutional layer
layer_conv_1d(
filters = 100,
kernel_size = 3,
padding = "valid",  # "valid" means no padding, as we did it already
activation = "relu",
strides = 2 ) %>%
layer_global_max_pooling_1d() %>%
layer_dense(128) %>%
layer_activation("relu") %>%
layer_dense(128) %>%
layer_dropout(0.5) %>%
layer_activation("relu") %>%
layer_dense(6) %>%   # single unit output layer
layer_activation("softmax")
summary(cnn_model)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "SGD",
metrics = "accuracy"
)
cnn_history <- cnn_model %>%
fit(
x_train, train$y,
batch_size = 400,
epochs = 10,
validation_data = list(x_test, test$y)
)
knitr::opts_chunk$set(echo = TRUE)
head(sentence_wide)
plot(cnn_history)
cnn_model %>% evaluate(x_test, test$y)
cnn_model <- keras_model_sequential()
cnn_model %>%
# embedding layer maps vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
# add some dropout
layer_dropout(0.5) %>%
# convolutional layer
layer_conv_1d(
filters = 100,
kernel_size = 3,
padding = "valid",  # "valid" means no padding, as we did it already
activation = "relu",
strides = 2 ) %>%
layer_global_max_pooling_1d() %>%
layer_dense(128) %>%
layer_activation("relu") %>%
layer_dense(128) %>%
layer_dropout(0.5) %>%
layer_activation("relu") %>%
layer_dense(6) %>%   # single unit output layer
layer_activation("softmax")
summary(cnn_model)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "Adamax",
metrics = "accuracy"
)
cnn_history <- cnn_model %>%
fit(
x_train, train$y,
batch_size = 400,
epochs = 10,
validation_data = list(x_test, test$y)
)
plot(cnn_history)
cnn_model %>% evaluate(x_test, test$y)
cnn_model <- keras_model_sequential()
cnn_model %>%
# embedding layer maps vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
# add some dropout
layer_dropout(0.5) %>%
# convolutional layer
layer_conv_1d(
filters = 100,
kernel_size = 3,
padding = "valid",  # "valid" means no padding, as we did it already
activation = "relu",
strides = 2 ) %>%
layer_global_max_pooling_1d() %>%
layer_dense(128) %>%
layer_activation("relu") %>%
layer_dense(128) %>%
layer_dropout(0.5) %>%
layer_activation("relu") %>%
layer_dense(6) %>%   # single unit output layer
layer_activation("softmax")
summary(cnn_model)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "Nadam",
metrics = "accuracy"
)
cnn_history <- cnn_model %>%
fit(
x_train, train$y,
batch_size = 400,
epochs = 10,
validation_data = list(x_test, test$y)
)
plot(cnn_history)
cnn_model %>% evaluate(x_test, test$y)
cnn_model %>% evaluate(x_test, test$y)
cnn_model <- keras_model_sequential()
cnn_model %>%
# embedding layer maps vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
# add some dropout
layer_dropout(0.5) %>%
# convolutional layer
layer_conv_1d(
filters = 100,
kernel_size = 3,
padding = "valid",  # "valid" means no padding, as we did it already
activation = "relu",
strides = 2 ) %>%
layer_global_max_pooling_1d() %>%
layer_dense(128) %>%
layer_activation("relu") %>%
layer_dense(128) %>%
layer_dropout(0.5) %>%
layer_activation("relu") %>%
layer_dense(6) %>%   # single unit output layer
layer_activation("softmax")
summary(cnn_model)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "Adadelta",
metrics = "accuracy"
)
cnn_history <- cnn_model %>%
fit(
x_train, train$y,
batch_size = 400,
epochs = 10,
validation_data = list(x_test, test$y)
)
plot(cnn_history)
cnn_model %>% evaluate(x_test, test$y)
cnn_model <- keras_model_sequential()
cnn_model %>%
# embedding layer maps vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
# add some dropout
layer_dropout(0.5) %>%
# convolutional layer
layer_conv_1d(
filters = 100,
kernel_size = 3,
padding = "valid",  # "valid" means no padding, as we did it already
activation = "relu",
strides = 2 ) %>%
layer_global_max_pooling_1d() %>%
layer_dense(256) %>%
layer_dropout(0.4) %>%
layer_activation("relu") %>%
layer_dense(128) %>%
layer_dropout(0.3) %>%
layer_activation("relu") %>%
layer_dense(6) %>%   # single unit output layer
layer_activation("softmax")
cnn_model %>%
# embedding layer maps vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
# add some dropout
layer_dropout(0.5) %>%
# convolutional layer
layer_conv_1d(
filters = 100,
kernel_size = 3,
padding = "valid",  # "valid" means no padding, as we did it already
activation = "relu",
strides = 2 ) %>%
layer_global_max_pooling_1d() %>%
layer_dense(256) %>%
layer_dropout(0.4) %>%
layer_activation("relu") %>%
layer_dense(128) %>%
layer_dropout(0.3) %>%
layer_activation("relu") %>%
layer_dense(6) %>%   # single unit output layer
layer_activation("softmax")
summary(cnn_model)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "Adam",
metrics = "accuracy"
)
cnn_history <- cnn_model %>%
fit(
x_train, train$y,
batch_size = 400,
epochs = 10,
validation_data = list(x_test, test$y)
)
cnn_model <- keras_model_sequential()
cnn_model %>%
# embedding layer maps vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
# add some dropout
layer_dropout(0.5) %>%
# convolutional layer
layer_conv_1d(
filters = 100,
kernel_size = 3,
padding = "valid",  # "valid" means no padding, as we did it already
activation = "relu",
strides = 2 ) %>%
layer_global_max_pooling_1d() %>%
layer_dense(256) %>%
layer_dropout(0.4) %>%
layer_activation("relu") %>%
layer_dense(128) %>%
layer_dropout(0.3) %>%
layer_activation("relu") %>%
layer_dense(6) %>%   # single unit output layer
layer_activation("softmax")
summary(cnn_model)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "Adam",
metrics = "accuracy"
)
cnn_history <- cnn_model %>%
fit(
x_train, train$y,
batch_size = 400,
epochs = 10,
validation_data = list(x_test, test$y)
)
plot(cnn_history)
cnn_model %>% evaluate(x_test, test$y)
cnn_model <- keras_model_sequential()
cnn_model %>%
# embedding layer maps vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
# add some dropout
layer_dropout(0.3) %>%
# convolutional layer
layer_conv_1d(
filters = 100,
kernel_size = 3,
padding = "valid",  # "valid" means no padding, as we did it already
activation = "relu",
strides = 2 ) %>%
layer_global_max_pooling_1d() %>%
layer_dense(256) %>%
layer_dropout(0.4) %>%
layer_activation("relu") %>%
layer_dense(128) %>%
layer_activation("relu") %>%
layer_dense(6) %>%   # single unit output layer
layer_activation("softmax")
summary(cnn_model)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "Adam",
metrics = "accuracy"
)
cnn_history <- cnn_model %>%
fit(
x_train, train$y,
batch_size = 400,
epochs = 10,
validation_data = list(x_test, test$y)
)
cnn_model <- keras_model_sequential()
cnn_model %>%
# embedding layer maps vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
# add some dropout
layer_dropout(0.5) %>%
# convolutional layer
layer_conv_1d(
filters = 100,
kernel_size = 3,
padding = "valid",  # "valid" means no padding, as we did it already
activation = "relu",
strides = 2 ) %>%
layer_global_max_pooling_1d() %>%
layer_dense(128) %>%
layer_activation("relu") %>%
layer_dense(128) %>%
layer_dropout(0.5) %>%
layer_activation("relu") %>%
layer_dense(6) %>%   # single unit output layer
layer_activation("softmax")
summary(cnn_model)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
cnn_history <- cnn_model %>%
fit(
x_train, train$y,
batch_size = 400,
epochs = 10,
validation_data = list(x_test, test$y)
)
cnn_history <- cnn_model %>%
fit(
x_train, train$y,
batch_size = 400,
epochs = 10,
validation_data = list(x_test, test$y)
)
plot(cnn_history)
cnn_model %>% evaluate(x_test, test$y)
cnn_model <- keras_model_sequential()
cnn_model %>%
# embedding layer maps vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
# add some dropout
layer_dropout(0.3) %>%
# convolutional layer
layer_conv_1d(
filters = 100,
kernel_size = 3,
padding = "valid",  # "valid" means no padding, as we did it already
activation = "relu",
strides = 2 ) %>%
layer_global_max_pooling_1d() %>%
layer_dense(256) %>%
layer_activation("relu") %>%
layer_dense(128) %>%
layer_dropout(0.5) %>%
layer_activation("relu") %>%
layer_dense(6) %>%   # single unit output layer
layer_activation("softmax")
summary(cnn_model)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
cnn_history <- cnn_model %>%
fit(
x_train, train$y,
batch_size = 400,
epochs = 10,
validation_data = list(x_test, test$y)
)
plot(cnn_history)
cnn_model %>% evaluate(x_test, test$y)
cnn_model <- keras_model_sequential()
cnn_model %>%
# embedding layer maps vocab indices into embedding_dims dimensions
layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
# add some dropout
layer_dropout(0.4) %>%
# convolutional layer
layer_conv_1d(
filters = 100,
kernel_size = 3,
padding = "valid",  # "valid" means no padding, as we did it already
activation = "relu",
strides = 2 ) %>%
layer_global_max_pooling_1d() %>%
layer_dense(256) %>%
layer_activation("relu") %>%
layer_dense(128) %>%
layer_dropout(0.3) %>%
layer_activation("relu") %>%
layer_dense(6) %>%   # single unit output layer
layer_activation("softmax")
summary(cnn_model)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
cnn_model %>% compile(
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = "accuracy"
)
cnn_history <- cnn_model %>%
fit(
x_train, train$y,
batch_size = 400,
epochs = 10,
validation_data = list(x_test, test$y)
)
plot(cnn_history)
cnn_model %>% evaluate(x_test, test$y)
