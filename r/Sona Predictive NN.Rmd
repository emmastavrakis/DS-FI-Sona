---
title: "Sona Text Analysis"
author: "Emma Stavrakis, Sarah Mbaka, Jaco de Swardt"
date: "20 September 2018"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#1.  Introduction

In this assignment we will be conducting both predictive and descriptive analyses on the SONA dataset.  To this end we have:

1.  Built a neural network and a convolutional neural network to predict which president was the source when given a sentence of text 
2.  Assessed the performance of both the nn and cnn on unseen data and then compared and interpreted these results
3.  Conducted a desriptive analysis of the text in the speeches

This report sets out the approach we took to conduct these analyses, decisions and choices that we made, what challenges we encountered and how we overcame these, as well as our interpretation of- and commentary on the results.

We were tasked to work in groups for this assignment and to advise how the workload was divided amongst the group members, as well as providing evidence of team collaboration on Git.  We have covered this section at the end of this workbook.  Our repo for this project may be accessed here:
https://github.com/emmastavrakis/DS-FI-Sona/
    

***


#2.  Data preparation

The State of the Nation Address of the President of South Africa (SONA) is an annual event in which the President of South Africa reports on the status of the nation, normally to the resumption of a joint sitting of Parliament. In years that elections took place, a State of the Nation Address happens twice, once before and again after the election.

We were provided with full text of all State of the Nation Address (SONA) speeches, from 1994 through to 2018 on which to conduct our analyses.

##2.1  Data challenges

The first challenge that we encountered was that some the data that was provided had inadvertantly been duplicated on kaggle (Mbeki's speech from 2007 and 2008 were found to be the same). We replaced the incorrect data with the correct version directly in the source data files for this project.

The next challenge that we encountered is that some of the data categories were very sparse, as some presidents had given many more SONA speeches than others.  Particularly, Presidents de Klerk, Mothlante and Ramaphosa had each only delivered one speech.  We debated reducing the sample size of the other presidents accordingly, but this would have resulted in ignoring 75% of the total dataset size, which we decided againt.  We debated over-sampling the sparse presidents, but some of the other groups advised that they had tried this already with very little impact on overall prediction accuracy.  So we decided to leave the data as-is, and rather to focus on tuning our models.


##2.2 Pre-processing

Our approach to pre-processing the data centred primarily on using the **tidyverse** and **tidytext** libraries to wrangle the data and mine the text.

We started by extracting metadata to identify the president who was the source of each speech, as well as the year that the speech was given.  Initially we labelled the column with the president's name as **"president"**, which caused problems later on with tokenization as the word "president" is also a token.  We then changed this column reference to **"which_pres"**.

```{r echo=FALSE}

suppressPackageStartupMessages(library(tidyverse))

# extract the filenames
txt_files <- list.files("../data/")
sona <- data.frame(filename = as.character(), speech = as.character())
for(i in txt_files){
  file_name <- paste0("../data/", i)
  
  # import text as single character string (can also read.table but the "seperator" causes problems)
  this_speech <- readChar(file_name, nchars = file.info(file_name)$size)
  
  # make data frame with metadata (filename contains year and pres) and speech
  this_sona <- data.frame(filename = i, speech = this_speech, stringsAsFactors = FALSE)
  
  # make a single dataset
  sona <- rbind(sona, this_sona)
}

# extract year
sona$year <- str_sub(sona$filename, start = 1, end = 4)

# extract president name
sona$which_pres <- sub('.*_', '', sona$filename)
sona$which_pres <- sub('*.txt', '', sona$which_pres)
```

Thereafter we tokenized the words and sentences, produced counts of each token and then spread the data from long format to wide format as required by the neural networks that we are building.  Below is a sample of the wrangled, tokenized, counted data, ready to be processed by the neural network.

```{r echo=FALSE}

# pre-processing data to get into right format for neural nets

suppressPackageStartupMessages(library(tidytext))


# word tokenization
tidy_sona <- sona %>% unnest_tokens(word, speech, token = "words", to_lower = T) %>% 
  filter(!word %in% stop_words$word) 

word_counts <- tidy_sona %>% group_by(filename, word) %>% count() 

# we want to predict sentences, so we need to first split into sentences
# add an ID variable for sentences and tokenize each sentence by words
tidy_sentences <- sona %>% 
  unnest_tokens(sentence, speech, token = "sentences", to_lower = T) %>%
  rowid_to_column("ID")

tidy_words <- tidy_sentences %>% 
  unnest_tokens(word, sentence, token = "words", to_lower = T) %>% 
  filter(!word %in% stop_words$word) 

# count how many times each word was used in each sentence
sentence_counts <- tidy_words %>% 
  group_by(ID, which_pres, word) %>% 
  count()

# reshape long to wide to get into usual format for predictive models 
# using "spread"
sentence_wide <- sentence_counts %>%
  spread(key = word, value = n)


head(sentence_wide)


```


##2.3  Data partitioning

In order to assess the performance of the predictive models, we partitioned the data into a training dataset and a validation dataset.  We then extracted the response variable, **"which_pres"** into its own dataframe and "one-hot" encoded it.  Below is an extract of the one-hot encoded response variable from the training dataset.

```{r echo=FALSE}
nrows <- nrow(sentence_wide)
ncols <- ncol(sentence_wide)

#Create a set of index values to be used to split the dataset into train and test
set.seed(123)
sentence_wide[is.na(sentence_wide)] = 0 #clean the NAs
train <- sample(1:nrows,size=nrows*0.8, replace=FALSE)
pres_col <- which( colnames(sentence_wide)=="which_pres" )

# isolate the response variable and separate into test and train
# use a lookup function for the column number for the president label
y_train <- as.matrix(sentence_wide[train,pres_col], ncol = 1)
y_test <- as.matrix(sentence_wide[-train,pres_col], ncol = 1)

# one hot encoding for response variable
library(listarrays)
y_train <- onehot(y_train)
y_test <- onehot(y_test)

head(y_train)

# drop the response varaible 
sentence_wide <- sentence_wide[,-pres_col]

#and separate the predictor variables into test and train datasets
x_train <- as.matrix(sentence_wide[train,1:(ncols-1)], ncol = (ncols-1))
x_test <- as.matrix(sentence_wide[-train,1:(ncols-1)], ncol = (ncols-1))

```

***

#3.  Predictive Anaysis

For the predictive analysis we decided to build two models and compare their performance.  We started with a basic neural network (nn) and then built a convolutional neural network (cnn).

##3.1  Neural Network

The **keras** library was used to build the neural network.


```{r echo=FALSE}
suppressPackageStartupMessages(library(keras))
#install_keras(method = "conda")

set.seed(123)
nn_model <- keras_model_sequential()

```


Neural networks provide a lot of flexibility in terms of their architecture and other parameters.  It can take a long time to define and tune an optimal model because there are so many variables involved and models can be very resource intensive, causing them to take a long time to run.

Some of the considerations that we took into account are summarised below:

#### Hyperparameter tuning
Tuning hyperparameters for deep neural network is difficult as it is slow to train a deep neural network and there are numerours parameters to configure. In this part, we briefly survey the hyperparameters for convnet.

##### Learning rate
Learning rate controls how much to update the weight in the optimization algorithm. We can use fixed learning rate, gradually decreasing learning rate, momentum based methods or adaptive learning rates, depending on our choice of optimizer such as SGD, Adam, Adagrad, AdaDelta or RMSProp.

##### Number of epochs
Number of epochs is the the number of times the entire training set pass through the neural network. We should increase the number of epochs until we see a small gap between the test error and the training error.

##### Batch size
Mini-batch is usually preferable in the learning process of convnet. A range of 16 to 128 is a good choice to test with. We should note that convnet is sensitive to batch size.

##### Activation function
Activation funtion introduces non-linearity to the model. Usually, rectifier works well with convnet. Other alternatives are sigmoid, tanh and other activation functions depening on the task.

##### Number of hidden layers and units
It is usually good to add more layers until the test error no longer improves. The trade off is that it is computationally expensive to train the network. Having a small amount of units may lead to underfitting while having more units are usually not harmful with appropriate regularization.


##### Dropout for regularization
Dropout is a preferable regularization technique to avoid overfitting in deep neural networks. The method simply drops out units in neural network according to the desired probability. A default value of 0.5 is a good choice to test with.


###3.1.1 Define the nn model architecture

We defined the nn model with the following architecture:

```{r}

nn_model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(ncols-1)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 6, activation = 'softmax')

```

which is represented by the following summary:

```{r echo=FALSE}

summary(nn_model)
```

###3.1.2  Compile the model

Before the neural network can be trained, it first needs to be compipled.  In this step we specified the loss type, optimisation algorithm and measurement metric.  Because our predictive model has more than two possible categories to be predicted, we used categorical cross-entropy for the loss function.  There are a number of optimizer algorithms that can be used, but for the purposes of a basic neural network we elected to use the **RMSProp** optimizer.

```{r echo=FALSE}

nn_model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'rmsprop',
  metrics = c('accuracy')
)

```

###3.1.3  Train the model

In training the model, it is necessary to specify the epochs, batch size and proportion of data to be used for validation.  We elected to use 20% of the data for validation, and then tuned the other hyper parameters manually to maximise the validation accuracy of the model.  We found that values of 50 epochs and a batch size of 400 gave optimal results.

```{r echo=FALSE}

# train the nn

set.seed(123)

nn_history <- nn_model %>% fit(
  x_train, y_train, 
  epochs = 50, batch_size = 400, 
  validation_split = 0.2
)


```

```{r echo=FALSE}
plot(nn_history)

```

###3.1.4  Evaluate the model on the unseen data

In order to assess the accuracy of the nn model, we ran it on our unseen dataset, where it achieved the following results:

```{r echo=FALSE}
nn_model %>% evaluate(x_test, y_test)
```


###3.1.5  Cross validation

We then performed 10-fold cross validation to further validate the results achieved by our testing.  Below is a plot of the resulting loss and accuracy measures produced by 10-fold cross validation.  This confirms the results achievd above.

```{r echo=FALSE}

suppressPackageStartupMessages(library('caret'))


loss=0
accuracy=0
val_loss=0
val_acc=0

set.seed(121)

kfold <- createFolds(1:nrow(x_train), 10)

for (i in 1:10) {

  
    crossval_history <- nn_model %>%
    fit(x_train[c(kfold[[i]]),],y_train[c(kfold[[i]]),],
        epochs=30,batch_size=400,
        validation_split=0.2)
    loss[i]<-crossval_history$metrics$loss
    accuracy[i]<-crossval_history$metrics$acc
    val_loss[i]<-crossval_history$metrics$val_loss
    val_acc[i]<-crossval_history$metrics$val_acc
  

}

plot(1:10,accuracy,type = "o",ylim = c(0,1))
lines(1:10,val_loss,type = "o", col="red")
#lines(1:10,loss, col="blue")
lines(1:10,val_acc,col="green")

```

###3.1.6  Generate predictions on new data

Lastly, we used the nn model to generate predictions for the held-back data in our test dataset.  A sample of these predictions is given below:

```{r echo=FALSE}

# predict on the unseen data
nn_predict <- nn_model %>% predict_classes(x_test)

head(nn_predict)

```

***

##3.2  Convolutional Neural Network

As an alternative approach for the predictive analysis, we also developed a convolutional neural network (cnn), usign the **keras** library.


###3.2.1 Additional data preparation for cnn

The cnn model requires that the data is consistently structured, which means that we need to pad the sentences that we feed into the model so that they are all the same length.  We have chosen a maximum length of 300, and we have ignored sentences of fewer than 5 words.  Additionally, we have limted the maximum features (most popular words) to 2000.

```{r}
max_features <- 2000        # choose max_features most popular words
minlen <- 5                # exclude sentences shorter than this
maxlen <- 300             # longest sentence (for padding)
embedding_dims <- 10       # number of dimensions for word embedding
```

We then tokenized the sentences according to the features and other variables that we've already defined, turning each sentence into a vector of integers, each integer representing a word, as follows:

```{r echo=FALSE}

#tokenize words and create sequences
tokenizer = text_tokenizer(num_words = max_features)
fit_text_tokenizer(tokenizer, tidy_sentences$sentence)
sequences = tokenizer$texts_to_sequences(tidy_sentences$sentence)
length(sequences)

head(tokenizer)
```


```{r echo=FALSE}

#split the data into test and training sets
tidy_sentences<-as.tibble(tidy_sentences)
tidy_sentences$which_pres<-as.numeric(as.factor(tidy_sentences$which_pres))

train_y_mat<-to_categorical(tidy_sentences$which_pres)
train_y_mat<-train_y_mat[,-1]
nrow(train_y_mat)
test <- list()
train <- list()

train_id <- sample(1:length(sequences),
                size = 0.8*length(sequences), 
                replace=F)

nrow(train$y)
test$x <-  sequences[-train_id]
train$x <- sequences[train_id]

train$y <- train_y_mat[train_id,]
test$y <-  train_y_mat[-train_id,]

table(train$y)
```

Sequences are of different lengths. We "pad" the shorter sequences with zeros so that all padded sequences are the same length.

```{r}
x_train <- train$x %>% pad_sequences(maxlen = maxlen)
x_test <- test$x %>% pad_sequences(maxlen = maxlen)
```


##3.2.2  Define the cnn model architecture

We defined our cnn model with the following architecture:

```{r echo=FALSE}
cnn_model <- keras_model_sequential()
```

```{r}
cnn_model %>% 
  # embedding layer maps vocab indices into embedding_dims dimensions
  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
  # add some dropout
  layer_dropout(0.5) %>%
  # convolutional layer
  layer_conv_1d(
    filters = 100,
    kernel_size = 3, 
    padding = "valid",  # "valid" means no padding, as we did it already
    activation = "relu", 
    strides = 2 ) %>%
    layer_global_max_pooling_1d() %>%
  layer_dense(128) %>%
  layer_activation("relu") %>%
  
  layer_dense(128) %>%
  layer_dropout(0.5) %>%
  layer_activation("relu") %>%
  
  layer_dense(6) %>%   # single unit output layer
  layer_activation("softmax")

```

The above architecture gives us a model that looks like this:

```{r echo=FALSE}
summary(cnn_model)
```

Although we experimented with different architectures, changing the number of layers, tuning the dropout rates etc., we found that the above architecture gave the best results.

Further details about the results given by other cnn architectures is available in the appendix.


###3.2.3  Compile the cnn model

Just like we did with the classic neural network earlier, we had to compile the cnn model before we can train it. In this step we specified the loss type, optimisation algorithm and measurement metric.  Again, we used categorical cross-entropy for the loss function. But we found that out of all of the optimizer algorithms available, the  **Nadam** optimizer gave the best results.

Further details about the results given by each optimizer is available in the appendix.

```{r echo=FALSE}
cnn_model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "Nadam",
  metrics = "accuracy"
)

```

###3.2.4  Train the cnn model

Just as we did for the classic neural network earlier, we had to specify the batch size and epochs for the training step, as well as the validation data.  

We used the same batch size of 400 for the cnn, as we had for the nn.  

However, we found that the much longer runtime for the cnn meant that we could not efficiently use a high number of epochs to run and tune our model.  We settled on 10 epochs, which is substantially less than the 50 epochs we used for the nn.



```{r echo = FALSE}

# cnn model training parameters
set.seed(123)

cnn_history <- cnn_model %>%
  fit(
    x_train, train$y,
    batch_size = 400,
    epochs = 10,
    validation_data = list(x_test, test$y)
  )
```

Our cnn model was able to achieve the following performance:

```{r echo=FALSE}
plot(cnn_history)

```

###3.2.5  Evaluate the cnn model on the unseen data

In order to assess the accuracy of the cnn model, we ran it on our unseen dataset, where it achieved the following results:


```{r echo=FALSE}
cnn_model %>% evaluate(x_test, test$y)
```

Further details about the tuning performed on these variables is available in the appendix.

***

#4.  Comparison of results


***

#5.  Descriptive Analysis


##5.1  Sentiment Analysis


##5.2  Topic Modelling

***

#7.  Team collaboration approach


***


#6.  Conclusion

***

#Appendix

