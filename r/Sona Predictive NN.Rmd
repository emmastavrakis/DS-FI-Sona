---
title: "Sona Predictive"
author: "Emma Stavrakis"
date: "09 September 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE}
library(tidyverse)

txt_files <- list.files("../data/")

sona <- data.frame(filename = as.character(), speech = as.character())
for(i in txt_files){
  file_name <- paste0("../data/", i)
  
  # import text as single character string (can also read.table but the "seperator" causes problems)
  this_speech <- readChar(file_name, 
                          nchars = file.info(file_name)$size)
  
  # make data frame with metadata (filename contains year and pres) and speech
  this_sona <- data.frame(filename = i, speech = this_speech, stringsAsFactors = FALSE)
  
  # make a single dataset
  sona <- rbind(sona, this_sona)
}

# extract year, testing
#str_extract(sona$filename, "[0-9]")
#str_extract_all(sona$filename, "[0-90-90-90-9]")
#str_extract_all(sona$filename, "[0-9][0-9][0-9][0-9]")
#str_extract_all(sona$filename, "[0-9][0-9][0-9][0-9]", simplify = T)
#str_extract(sona$filename, "[0-9]{4}")

# does the same thing
#str_sub(sona$filename, start = 1, end = 4)

sona$year <- str_sub(sona$filename, start = 1, end = 4)

# Extract president name
sona$president <- sub('.*_', '', sona$filename)
sona$president <- sub('*.txt', '', sona$president)

# pre-processing data to get into right format for neural nets

library(tidytext)

# unnest_tokens is very useful, can split into words, sentences, lines, paragraphs, etc

# word tokenization
sona %>% unnest_tokens(text, speech, token = "words")


#tidy_sona <- sona %>% unnest_tokens(text, speech, token = "sentences")

tidy_sona <- sona %>% unnest_tokens(word, speech, token = "words", to_lower = T) %>% 
  filter(!word %in% stop_words$word) 

word_counts <- tidy_sona %>% group_by(filename, word) %>% count() 

# we want to predict sentences, so we need to first split into sentences
# Add an ID variable for sentences and tokenize each sentence by words
tidy_sentences <- sona %>% 
  unnest_tokens(sentence, speech, token = "sentences", to_lower = T) %>%
  rowid_to_column("ID")

tidy_words <- tidy_sentences %>% 
  unnest_tokens(word, sentence, token = "words", to_lower = T) %>% 
  filter(!word %in% stop_words$word) 

# Count how many times each word was used in each sentence
sentence_counts <- tidy_words %>% group_by(ID, word) %>% count() 

# exercise: reshape long to wide to get into usual format for predictive models 
# using "spread"




```